{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Dv8absVKufcA",
   "metadata": {
    "id": "Dv8absVKufcA"
   },
   "source": [
    "# Semantic Segmentation with Deep Learning: Training and Testing on Colab\n",
    "\n",
    "Insert the following Javascript snippet into your browser console so that your Colab runtime won't time out. Open developer-settings (in your web-browser) with Ctrl+Shift+I then click on console tab and type this on the console prompt. (for mac press Option+Command+I)\n",
    "```Javascript\n",
    "function ClickConnect(){\n",
    "    console.log(\"Clicked on connect button\"); \n",
    "    document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdweXW5Xqd6R",
   "metadata": {
    "id": "bdweXW5Xqd6R"
   },
   "source": [
    "Zip up your code locally with `python zip_for_colab.py`, and upload your `cv_proj5.zip` file. Hit refresh, then run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ah8PNwYTqM1G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ah8PNwYTqM1G",
    "outputId": "9c29b07d-0639-462a-a280-87196052283e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cv_proj5_colab.zip\n",
      "replace pyproject.toml? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip cv_proj5_colab.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0pf627lnqsTo",
   "metadata": {
    "id": "0pf627lnqsTo"
   },
   "source": [
    "Install the `proj6_code` module locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sEkEfbqNqxa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEkEfbqNqxa4",
    "outputId": "0fa88998-700a-433b-8a11-9a3ee2aa5f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mCamvid\u001b[m\u001b[m                 \u001b[1m\u001b[36mexp\u001b[m\u001b[m                    setup.cfg\n",
      "README.md              \u001b[1m\u001b[36minitmodel\u001b[m\u001b[m              \u001b[1m\u001b[36msrc\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mconda\u001b[m\u001b[m                  \u001b[1m\u001b[36mkitti\u001b[m\u001b[m                  \u001b[1m\u001b[36mtests\u001b[m\u001b[m\n",
      "cv_proj5_colab.zip     kitti.zip              \u001b[1m\u001b[36mvision\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mdataset_lists\u001b[m\u001b[m          proj5_colab.ipynb      vpokharna3.zip\n",
      "\u001b[1m\u001b[36mdocs\u001b[m\u001b[m                   proj5_kitti_test.ipynb zip_for_colab.py\n",
      "\u001b[31mdownload_dataset.sh\u001b[m\u001b[m    proj5_local.ipynb      zip_submission.py\n",
      "\u001b[31mdownload_dataset.sh-e\u001b[m\u001b[m  pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-franchise",
   "metadata": {
    "id": "sensitive-franchise"
   },
   "source": [
    "Download ImageNet-pretrained ResNet-50:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bound-explosion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bound-explosion",
    "outputId": "44715d2e-2c34-4731-ab3a-4623dbf31a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: initmodel: File exists\n"
     ]
    }
   ],
   "source": [
    "!wget -O \"resnet50_v2.pth\" --no-check-certificate 'https://docs.google.com/uc?export=download&id=1w5pRmLJXvmQQA5PtCbHhZc_uC4o0YbmA'\n",
    "!mkdir initmodel && mv resnet50_v2.pth initmodel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "yZDeFtlyuXNz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZDeFtlyuXNz",
    "outputId": "95b172dd-8447-43eb-c45a-b7ec3c80ea35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "# The ImageNet-pretrained ResNet-50 weights should be 99 MB\n",
    "!ls -ltrh initmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7wzfFzyHupog",
   "metadata": {
    "id": "7wzfFzyHupog"
   },
   "source": [
    "Download the Camvid dataset images. It's 700 MB, but it should only take 30 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intellectual-delaware",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "intellectual-delaware",
    "outputId": "da7a478b-0887-4f7c-dbce-f823eac18653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camvid will be downloaded to Camvid\n",
      "Downloading Camvid dataset...\n",
      "./download_dataset.sh: line 23: wget: command not found\n",
      "Camvid dataset downloaded.\n",
      "Extracting Camvid dataset...\n",
      "unzip:  cannot find or open 701_StillsRaw_full.zip, 701_StillsRaw_full.zip.zip or 701_StillsRaw_full.zip.ZIP.\n",
      "Camvid dataset extracted.\n"
     ]
    }
   ],
   "source": [
    "!chmod +rwx download_dataset.sh\n",
    "!sed -i -e 's/\\r$//' download_dataset.sh\n",
    "!./download_dataset.sh Camvid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "PGBUoTc9Aj0t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGBUoTc9Aj0t",
    "outputId": "c88cd1e5-7939-49ad-a89c-d41feaa9ef73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mCamvid\u001b[m\u001b[m                 \u001b[1m\u001b[36mexp\u001b[m\u001b[m                    setup.cfg\n",
      "README.md              \u001b[1m\u001b[36minitmodel\u001b[m\u001b[m              \u001b[1m\u001b[36msrc\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mconda\u001b[m\u001b[m                  \u001b[1m\u001b[36mkitti\u001b[m\u001b[m                  \u001b[1m\u001b[36mtests\u001b[m\u001b[m\n",
      "cv_proj5_colab.zip     kitti.zip              \u001b[1m\u001b[36mvision\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mdataset_lists\u001b[m\u001b[m          proj5_colab.ipynb      vpokharna3.zip\n",
      "\u001b[1m\u001b[36mdocs\u001b[m\u001b[m                   proj5_kitti_test.ipynb zip_for_colab.py\n",
      "\u001b[31mdownload_dataset.sh\u001b[m\u001b[m    proj5_local.ipynb      zip_submission.py\n",
      "\u001b[31mdownload_dataset.sh-e\u001b[m\u001b[m  pyproject.toml\n",
      "Archive:  camvid_semseg11.zip\n",
      "replace semseg11/0016E5_08085_L.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!cd Camvid && unzip camvid_semseg11.zip && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AC_-gfRptGgF",
   "metadata": {
    "id": "AC_-gfRptGgF"
   },
   "source": [
    "We'll now set some default hyperparameters for training. Choose the number of epochs you'd like to train for (for PSPNet, it will take ~30 min for 50 epochs, or ~70 min for 100 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absent-major",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "absent-major",
    "outputId": "5c0fd99f-f7ee-48f1-f445-59da6d42d2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    **{\n",
    "        # DATA\n",
    "        \"names_path\": \"./dataset_lists/camvid-11/camvid-11_names.txt\",\n",
    "        \"data_root\": \"./Camvid/\",\n",
    "        \"train_list\": \"./src/dataset_lists/camvid-11/list/train.txt\",  \n",
    "        \"val_list\": \"./src/dataset_lists/camvid-11/list/val.txt\",\n",
    "        \"classes\": 11,\n",
    "        # TRAIN\n",
    "        \"arch\": \"PSPNet\", #  \"SimpleSegmentationNet\", # \n",
    "        \"save_path\": \"\",\n",
    "        \"epochs\": 5,\n",
    "        \"zoom_factor\": 8,\n",
    "        \"use_ppm\": False,\n",
    "        \"aux_weight\": 0.4,\n",
    "        \"aux_loss\": False,\n",
    "        \"layers\": 50,\n",
    "        \"workers\": 2,\n",
    "        \"batch_size\": 32,\n",
    "        \"batch_size_val\": 32,\n",
    "        \"data_aug\": True,\n",
    "        \"short_size\": 240,\n",
    "        \"train_h\": 201,\n",
    "        \"train_w\": 201,\n",
    "        \"init_weight\": \"./initmodel/resnet50_v2.pth\",\n",
    "        \"scale_min\": 0.5,  # minimum random scale\n",
    "        \"scale_max\": 2.0,  # maximum random scale\n",
    "        \"rotate_min\": -10,  # minimum random rotate\n",
    "        \"rotate_max\": 10,  # maximum random rotate\n",
    "        \"ignore_label\": 255,\n",
    "        \"base_lr\": 0.01,\n",
    "        \"start_epoch\": 0,\n",
    "        \"power\": 0.9,\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"manual_seed\": 0,\n",
    "        \"print_freq\": 10,\n",
    "        \"save_freq\": 1,\n",
    "        \"evaluate\": True,  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend\n",
    "        \"multiprocessing_distributed\": False,\n",
    "        # INFERENCE\n",
    "        \"dataset\": \"camvid-11\",\n",
    "        \"base_size\": 240,\n",
    "        \"test_h\": 201,\n",
    "        \"test_w\": 201,\n",
    "        \"scales\": [1.0], # [0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
    "        \"test_list\": \"./src/dataset_lists/camvid-11/list/val.txt\",\n",
    "        \"vis_freq\": 10,\n",
    "        \"pretrained\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "args.save_path = f\"exp/camvid/{args.arch}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "increased-blade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "increased-blade",
    "outputId": "a6023f1e-e981-4496-c2d9-2299ef7610b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(names_path='./dataset_lists/camvid-11/camvid-11_names.txt', data_root='./Camvid/', train_list='./src/dataset_lists/camvid-11/list/train.txt', val_list='./src/dataset_lists/camvid-11/list/val.txt', classes=11, arch='PSPNet', save_path='exp/camvid/PSPNet/model', epochs=5, zoom_factor=8, use_ppm=False, aux_weight=0.4, aux_loss=False, layers=50, workers=2, batch_size=32, batch_size_val=32, data_aug=True, short_size=240, train_h=201, train_w=201, init_weight='./initmodel/resnet50_v2.pth', scale_min=0.5, scale_max=2.0, rotate_min=-10, rotate_max=10, ignore_label=255, base_lr=0.01, start_epoch=0, power=0.9, momentum=0.9, weight_decay=0.0001, manual_seed=0, print_freq=10, save_freq=1, evaluate=True, multiprocessing_distributed=False, dataset='camvid-11', base_size=240, test_h=201, test_w=201, scales=[1.0], test_list='./src/dataset_lists/camvid-11/list/val.txt', vis_freq=10, pretrained=True)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './initmodel/resnet50_v2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main_worker\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmain_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/georgia-tech/spring24/cs4476-shi-vp/assignment-5/src/vision/trainer.py:58\u001b[0m, in \u001b[0;36mmain_worker\u001b[0;34m(args, use_cuda)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain_worker\u001b[39m(args, use_cuda: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     model, optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_and_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(args)\n\u001b[1;32m     60\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=> creating model ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/georgia-tech/spring24/cs4476-shi-vp/assignment-5/src/vision/part3_training_utils.py:45\u001b[0m, in \u001b[0;36mget_model_and_optimizer\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     37\u001b[0m     parameter_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     38\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mlayer0\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mbase_lr},\n\u001b[1;32m     39\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mresnet\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mbase_lr},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mresnet\u001b[38;5;241m.\u001b[39mlayer4\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mbase_lr},\n\u001b[1;32m     43\u001b[0m     ]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mPSPNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzoom_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     parameter_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     52\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mlayer0\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mbase_lr},\n\u001b[1;32m     53\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mbase_lr},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39maux\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mbase_lr \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m},\n\u001b[1;32m     60\u001b[0m     ]\n\u001b[1;32m     61\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[1;32m     62\u001b[0m     parameter_list, weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mweight_decay, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum\n\u001b[1;32m     63\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/georgia-tech/spring24/cs4476-shi-vp/assignment-5/src/vision/part5_pspnet.py:291\u001b[0m, in \u001b[0;36mPSPNet.__init__\u001b[0;34m(self, layers, bins, dropout, num_classes, zoom_factor, use_ppm, criterion, pretrained, deep_base)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m resnet \u001b[38;5;241m=\u001b[39m \u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer0 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    293\u001b[0m     resnet\u001b[38;5;241m.\u001b[39mconv1,\n\u001b[1;32m    294\u001b[0m     resnet\u001b[38;5;241m.\u001b[39mbn1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m     resnet\u001b[38;5;241m.\u001b[39mmaxpool,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1 \u001b[38;5;241m=\u001b[39m resnet\u001b[38;5;241m.\u001b[39mlayer1\n",
      "File \u001b[0;32m~/Desktop/georgia-tech/spring24/cs4476-shi-vp/assignment-5/src/vision/resnet.py:205\u001b[0m, in \u001b[0;36mresnet50\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./initmodel/resnet50_v2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 205\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/cv_proj5/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/cv_proj5/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/cv_proj5/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './initmodel/resnet50_v2.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "from src.vision.trainer import main_worker\n",
    "print(args)\n",
    "main_worker(args, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7or_wjTqvX6H",
   "metadata": {
    "id": "7or_wjTqvX6H"
   },
   "source": [
    "We'll now create full-resolution predictions for the full val set, and compute mIoU against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-vegetation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "worst-vegetation",
    "outputId": "12e70bf3-c7d7-4181-a781-f617ff7999cb"
   },
   "outputs": [],
   "source": [
    "from vision.test import test_model\n",
    "args.model_path = f\"exp/camvid/{args.arch}/model/train_epoch_{args.epochs}.pth\"\n",
    "test_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ETWCIkf1vfCP",
   "metadata": {
    "id": "ETWCIkf1vfCP"
   },
   "source": [
    "**Important**: Record the mIoU listed in the output above, and the IoU per each class. You can find the results later in `train_epoch_{args.epochs}/camvid-11/720/results.txt`.\n",
    "\n",
    "Now, let's take a look at what our results look like. We'll make a 2x3 image grid with the following structure:\n",
    "\n",
    "|RGB Image | Blended RGB and Ground Truth | Ground Truth \n",
    "|:-: | :-: | :-:\n",
    "| RGB Image | Blended RGB and Prediction | Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cDpIrDQvvBq5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "cDpIrDQvvBq5",
    "outputId": "bc1be37a-86ea-447c-ef36-1adf53e6033a"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rgb_predictions_dir = f\"train_epoch_{args.epochs}/camvid-11/{args.base_size}/rgb_mask_predictions\"\n",
    "\n",
    "def show_image_grid(rgb_predictions_dir: str, img_fname: str) -> None:\n",
    "  img_grid = imageio.imread(f'{rgb_predictions_dir}/{img_fname}')\n",
    "  plt.figure(figsize=(15,7))\n",
    "  plt.imshow(img_grid)\n",
    "  plt.show()\n",
    "\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_07977.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JOxOOpJ-wDHa",
   "metadata": {
    "id": "JOxOOpJ-wDHa"
   },
   "source": [
    "We'll look at more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wJo0THuZvDkU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wJo0THuZvDkU",
    "outputId": "3af4b638-fed6-4f66-c29b-d65a75524e25"
   },
   "outputs": [],
   "source": [
    "show_image_grid(rgb_predictions_dir, \"0016E5_07997.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08017.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08037.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08057.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08077.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08097.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08117.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08137.jpg\")\n",
    "show_image_grid(rgb_predictions_dir, \"0016E5_08157.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VFCSB5B23t19",
   "metadata": {
    "id": "VFCSB5B23t19"
   },
   "source": [
    "Now, zip up your predictions on the test set for your best model, **download them locally to your machine**, and submit these to Gradescope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VbYbqcNn3eS2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbYbqcNn3eS2",
    "outputId": "5596b314-cbd0-43d0-c5cf-d767d6a66bd7"
   },
   "outputs": [],
   "source": [
    "grayscale_predictions_dir = f\"train_epoch_{args.epochs}/camvid-11/{args.base_size}/gray\"\n",
    "!ls -ltrh $grayscale_predictions_dir\n",
    "!zip -r grayscale_predictions.zip $grayscale_predictions_dir\n",
    "!ls -ltrh grayscale_predictions.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DBuC3SzAlQcU",
   "metadata": {
    "id": "DBuC3SzAlQcU"
   },
   "source": [
    "\n",
    "**Transfer Learning:** \n",
    "Zip the Kitti dataset and upload it to colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IJ6oQCX--9Xd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJ6oQCX--9Xd",
    "outputId": "2c846383-86f0-410e-8a6f-e1ba22c21da0"
   },
   "outputs": [],
   "source": [
    "!unzip kitti.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ib-UgbbzCXVI",
   "metadata": {
    "id": "ib-UgbbzCXVI"
   },
   "source": [
    "Load the model trained on the Camvid dataset. Change to your best model if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3UJ1porI_f0J",
   "metadata": {
    "id": "3UJ1porI_f0J"
   },
   "outputs": [],
   "source": [
    "args.model_path = f\"exp/camvid/{args.arch}/model/train_epoch_{args.epochs}.pth\"\n",
    "# args.model_path = f\"exp/camvid/{args.arch}/model/train_epoch_200.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2FXFsF3Msg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a2FXFsF3Msg",
    "outputId": "2a0accd9-b8ef-4ccc-b2a7-bd8cee42845d"
   },
   "outputs": [],
   "source": [
    "args.data_root = \"./kitti\"\n",
    "args.classes = 2\n",
    "args.save_path = f\"exp/kitti/{args.arch}/model\"\n",
    "args.batch_size = 32\n",
    "args.batch_size_val = 1\n",
    "args.dataset = \"kitti\"\n",
    "args.evaluate = False\n",
    "args.epochs = 10\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CdrbddFtHSw6",
   "metadata": {
    "id": "CdrbddFtHSw6"
   },
   "outputs": [],
   "source": [
    "args.base_lr = 0.01\n",
    "args.momentum = 0.9\n",
    "args.weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8qtSkys6eyNb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qtSkys6eyNb",
    "outputId": "003e2e84-a538-4539-f4cd-d439dc6f2e9c"
   },
   "outputs": [],
   "source": [
    "from src.vision.trainer import transfer_train\n",
    "transfer_train(args, torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "proj5_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
